{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0792a10-e46c-4e61-8ed6-1ffe2125aee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import string\n",
    "import ast\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Add, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Config ===\n",
    "MAX_LEN = 64\n",
    "EMBED_DIM = 64\n",
    "LSTM_UNITS = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# === Vocab (ASCII + control tokens) ===\n",
    "special_tokens = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
    "vocab_chars = list(string.ascii_lowercase + string.digits + string.punctuation + ' ')\n",
    "vocab = special_tokens + vocab_chars\n",
    "token_to_id = {c: i for i, c in enumerate(vocab)}\n",
    "id_to_token = {i: c for c, i in token_to_id.items()}\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token_to_id.get(c, token_to_id['<UNK>']) for c in text.lower()]\n",
    "\n",
    "def detokenize(ids):\n",
    "    return ''.join([id_to_token.get(i, '?') for i in ids])\n",
    "\n",
    "# === Load and preprocess dataset ===\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Parse list columns\n",
    "    df['personas'] = df['personas'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "    df['previous_utterance'] = df['previous_utterance'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "    df['free_messages'] = df['free_messages'].apply(lambda x: ast.literal_eval(x)[0] if pd.notnull(x) else \"\")\n",
    "\n",
    "    df['input_text'] = df.apply(lambda row: \" \".join(row['personas'] + row['previous_utterance']), axis=1)\n",
    "    df['target_text'] = df['free_messages']\n",
    "\n",
    "    # Filter empty targets\n",
    "    df = df[df['target_text'].str.len() > 0]\n",
    "\n",
    "    inputs, targets = [], []\n",
    "\n",
    "    for input_txt, target_txt in zip(df['input_text'], df['target_text']):\n",
    "        input_ids = tokenize(input_txt)[:MAX_LEN]\n",
    "        target_ids = [token_to_id['<BOS>']] + tokenize(target_txt)[:MAX_LEN - 2] + [token_to_id['<EOS>']]\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        targets.append(target_ids)\n",
    "\n",
    "    X = pad_sequences(inputs, maxlen=MAX_LEN, padding='post', value=token_to_id['<PAD>'])\n",
    "    Y = pad_sequences(targets, maxlen=MAX_LEN, padding='post', value=token_to_id['<PAD>'])\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# === Positional Embedding Layer ===\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        pos = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
    "        pos = self.pos_emb(pos)\n",
    "        return x + pos\n",
    "\n",
    "# === Attention ===\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, enc_out, dec_hidden):\n",
    "        # enc_out: (batch, time, hidden), dec_hidden: (batch, hidden)\n",
    "        dec_hidden_expanded = tf.expand_dims(dec_hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_out) + self.W2(dec_hidden_expanded)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * enc_out\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# === Build Model ===\n",
    "def build_model():\n",
    "    encoder_inputs = Input(shape=(MAX_LEN,), name='encoder_input')\n",
    "    x = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(encoder_inputs)\n",
    "    x = PositionalEmbedding(MAX_LEN, EMBED_DIM)(x)\n",
    "    lstm_out, state_h, state_c = LSTM(LSTM_UNITS, return_sequences=True, return_state=True)(x)\n",
    "\n",
    "    decoder_inputs = Input(shape=(MAX_LEN,), name='decoder_input')\n",
    "    x2 = Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)(decoder_inputs)\n",
    "    x2 = PositionalEmbedding(MAX_LEN, EMBED_DIM)(x2)\n",
    "    decoder_lstm = LSTM(LSTM_UNITS, return_sequences=True, return_state=False)\n",
    "    decoder_out = decoder_lstm(x2)\n",
    "\n",
    "    attention = BahdanauAttention(LSTM_UNITS)\n",
    "    context_vector, _ = attention(lstm_out, state_h)\n",
    "\n",
    "    context_repeated = tf.expand_dims(context_vector, 1)\n",
    "    repeated = tf.repeat(context_repeated, repeats=MAX_LEN, axis=1)\n",
    "\n",
    "    concat = Concatenate()([repeated, decoder_out])\n",
    "    output = Dense(VOCAB_SIZE, activation='softmax')(concat)\n",
    "\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# === Main training workflow ===\n",
    "def main():\n",
    "    print(\"ðŸš€ Loading and processing data...\")\n",
    "    X, Y = load_data(\"your_dataset.csv\")\n",
    "    Y_out = np.expand_dims(Y, -1)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y_out, test_size=0.1)\n",
    "\n",
    "    print(\"ðŸ§  Building model...\")\n",
    "    model = build_model()\n",
    "\n",
    "    print(\"ðŸŽ¯ Training...\")\n",
    "    model.fit([X_train, X_train], Y_train, validation_data=([X_val, X_val], Y_val),\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "    print(\"ðŸ’¾ Saving model...\")\n",
    "    model.save(\"model.h5\")\n",
    "\n",
    "    print(\"ðŸ§¬ Saving tokenizer...\")\n",
    "    with open(\"tokenizer.json\", \"w\") as f:\n",
    "        json.dump(token_to_id, f)\n",
    "\n",
    "    print(\"âœ… All done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddc1f9-8826-4eb0-a7b0-f7057cdb7513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
